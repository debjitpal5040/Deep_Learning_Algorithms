{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import exp, nn, no_grad, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms.v2 import ToTensor, Compose, Resize, Normalize, CenterCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../alzheimers_dataset/train\"\n",
    "TEST_PATH = \"../alzheimers_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose(\n",
    "    [Resize((256, 256)), CenterCrop(224), ToTensor(), Normalize([0.5], [0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageFolder(root=TRAIN_PATH, transform=transforms)\n",
    "test_set = ImageFolder(root=TEST_PATH, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True)\n",
    "test_loader = DataLoader(test_set, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 5121\n",
      "    Root location: ../alzheimers_dataset/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=warn)\n",
      "                 CenterCrop(size=(224, 224))\n",
      "                 ToTensor()\n",
      "                 Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n"
     ]
    }
   ],
   "source": [
    "print(train_set.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5121\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1279\n",
      "    Root location: ../alzheimers_dataset/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=warn)\n",
      "                 CenterCrop(size=(224, 224))\n",
      "                 ToTensor()\n",
      "                 Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n"
     ]
    }
   ],
   "source": [
    "print(test_set.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/h3xf46_564xddxy0lr8jzc2m0000gn/T/ipykernel_60683/2569669039.py:4: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtight_layout()\n\u001b[0;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mimshow(example_data[i][\u001b[39m0\u001b[39m], cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m, interpolation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mGround Truth: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(example_targets[i]))\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mxticks([])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot images from training set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 90000\n",
    "hidden_sizes = [9000, 900, 90]\n",
    "output_size = 4\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(hidden_sizes[2], output_size),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)  # log probabilities\n",
    "loss = criterion(logps, labels)  # calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "\n",
    "        # And optimizes its weights here\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\n",
    "            \"Epoch {} - Training loss: {}\".format(e, running_loss / len(train_loader))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "    \"\"\"Function for viewing an image and it's predicted classes.\"\"\"\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6, 9), ncols=2)\n",
    "    ax1.imshow(img.resize_(208, 176, 1).numpy().squeeze())\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title(\"Class Probability\")\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "img = images[0].view(1, 109824)\n",
    "with no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(3, 208, 176), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images, labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 109824)\n",
    "        with no_grad():\n",
    "            logps = model(img)\n",
    "        ps = exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if true_label == pred_label:\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count / all_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
