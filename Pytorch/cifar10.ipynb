{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Conv2d, CrossEntropyLoss, Linear, MaxPool2d, Module\n",
    "from torch.nn.functional import relu\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms.v2 import Compose, Grayscale, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation to apply to the data\n",
    "transform = Compose([ToTensor(), Grayscale(num_output_channels=1)])\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "trainset = CIFAR10(\"cifar10_data\", download=True, train=True, transform=transform)\n",
    "testset = CIFAR10(\"cifar10_data\", download=True, train=False, transform=transform)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Define the data loaders for the training, validation, and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3136]' is invalid for input of size 262144",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[39m# Forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n\u001b[1;32m     14\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[0;32m---> 15\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m64\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m7\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m7\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3136]' is invalid for input of size 262144"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(\n",
    "            device, dtype=torch.float32\n",
    "        )  # convert input tensor to float32\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy and loss\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print statistics\n",
    "        if i % 100 == 99:\n",
    "            # Compute validation accuracy and loss\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in valloader:\n",
    "                    images, labels = data\n",
    "                    images = images.to(\n",
    "                        device, dtype=torch.float32\n",
    "                    )  # convert input tensor to float32\n",
    "                    labels = labels.to(device)\n",
    "                    images, labels = data\n",
    "                    images = images.to(\n",
    "                        device, dtype=torch.float32\n",
    "                    )  # convert input tensor to float32\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch + 1}, Batch {i + 1}] \"\n",
    "                f\"Train Loss: {running_loss / 100:.3f} \"\n",
    "                f\"Train Acc: {100 * correct_train / total_train:.2f}% \"\n",
    "                f\"Val Loss: {val_loss / len(valloader):.3f} \"\n",
    "                f\"Val Acc: {100 * correct_val / total_val:.2f}%\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.032 Test Acc: 99.03%\n"
     ]
    }
   ],
   "source": [
    "# Compute test accuracy and loss\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {test_loss / len(testloader):.3f} \"\n",
    "    f\"Test Acc: {100 * correct_test / total_test:.2f}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
