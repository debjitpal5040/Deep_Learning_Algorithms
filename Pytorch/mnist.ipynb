{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Conv2d, CrossEntropyLoss, Linear, MaxPool2d, Module\n",
    "from torch.nn.functional import relu\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.v2 import Compose, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation to apply to the data\n",
    "transform = Compose([ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = MNIST(\"mnist_data\", download=True, train=True, transform=transform)\n",
    "testset = MNIST(\"mnist_data\", download=True, train=False, transform=transform)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Define the data loaders for the training, validation, and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a random image from the training set\n",
    "image, label = trainset[0]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] Train Loss: 0.720 Train Acc: 78.62% Val Loss: 0.294 Val Acc: 91.42%\n",
      "[Epoch 1, Batch 200] Train Loss: 0.228 Train Acc: 93.38% Val Loss: 0.234 Val Acc: 91.97%\n",
      "[Epoch 1, Batch 300] Train Loss: 0.149 Train Acc: 95.52% Val Loss: 0.125 Val Acc: 96.04%\n",
      "[Epoch 1, Batch 400] Train Loss: 0.113 Train Acc: 96.58% Val Loss: 0.109 Val Acc: 96.69%\n",
      "[Epoch 1, Batch 500] Train Loss: 0.099 Train Acc: 97.00% Val Loss: 0.096 Val Acc: 96.99%\n",
      "[Epoch 1, Batch 600] Train Loss: 0.078 Train Acc: 97.61% Val Loss: 0.082 Val Acc: 97.65%\n",
      "[Epoch 1, Batch 700] Train Loss: 0.076 Train Acc: 97.69% Val Loss: 0.073 Val Acc: 97.80%\n",
      "[Epoch 2, Batch 100] Train Loss: 0.065 Train Acc: 97.86% Val Loss: 0.067 Val Acc: 97.95%\n",
      "[Epoch 2, Batch 200] Train Loss: 0.060 Train Acc: 98.12% Val Loss: 0.074 Val Acc: 97.66%\n",
      "[Epoch 2, Batch 300] Train Loss: 0.059 Train Acc: 98.25% Val Loss: 0.069 Val Acc: 97.97%\n",
      "[Epoch 2, Batch 400] Train Loss: 0.051 Train Acc: 98.47% Val Loss: 0.056 Val Acc: 98.26%\n",
      "[Epoch 2, Batch 500] Train Loss: 0.055 Train Acc: 98.42% Val Loss: 0.067 Val Acc: 97.82%\n",
      "[Epoch 2, Batch 600] Train Loss: 0.052 Train Acc: 98.34% Val Loss: 0.054 Val Acc: 98.41%\n",
      "[Epoch 2, Batch 700] Train Loss: 0.048 Train Acc: 98.42% Val Loss: 0.049 Val Acc: 98.52%\n",
      "[Epoch 3, Batch 100] Train Loss: 0.044 Train Acc: 98.64% Val Loss: 0.071 Val Acc: 97.86%\n",
      "[Epoch 3, Batch 200] Train Loss: 0.042 Train Acc: 98.58% Val Loss: 0.061 Val Acc: 98.01%\n",
      "[Epoch 3, Batch 300] Train Loss: 0.042 Train Acc: 98.62% Val Loss: 0.053 Val Acc: 98.42%\n",
      "[Epoch 3, Batch 400] Train Loss: 0.038 Train Acc: 98.89% Val Loss: 0.053 Val Acc: 98.34%\n",
      "[Epoch 3, Batch 500] Train Loss: 0.029 Train Acc: 99.08% Val Loss: 0.045 Val Acc: 98.59%\n",
      "[Epoch 3, Batch 600] Train Loss: 0.042 Train Acc: 98.72% Val Loss: 0.041 Val Acc: 98.77%\n",
      "[Epoch 3, Batch 700] Train Loss: 0.033 Train Acc: 98.98% Val Loss: 0.048 Val Acc: 98.50%\n",
      "[Epoch 4, Batch 100] Train Loss: 0.022 Train Acc: 99.27% Val Loss: 0.048 Val Acc: 98.62%\n",
      "[Epoch 4, Batch 200] Train Loss: 0.024 Train Acc: 99.31% Val Loss: 0.046 Val Acc: 98.49%\n",
      "[Epoch 4, Batch 300] Train Loss: 0.030 Train Acc: 98.98% Val Loss: 0.047 Val Acc: 98.58%\n",
      "[Epoch 4, Batch 400] Train Loss: 0.026 Train Acc: 99.11% Val Loss: 0.050 Val Acc: 98.47%\n",
      "[Epoch 4, Batch 500] Train Loss: 0.029 Train Acc: 99.00% Val Loss: 0.048 Val Acc: 98.54%\n",
      "[Epoch 4, Batch 600] Train Loss: 0.024 Train Acc: 99.20% Val Loss: 0.055 Val Acc: 98.55%\n",
      "[Epoch 4, Batch 700] Train Loss: 0.035 Train Acc: 98.83% Val Loss: 0.053 Val Acc: 98.33%\n",
      "[Epoch 5, Batch 100] Train Loss: 0.015 Train Acc: 99.59% Val Loss: 0.044 Val Acc: 98.69%\n",
      "[Epoch 5, Batch 200] Train Loss: 0.023 Train Acc: 99.38% Val Loss: 0.049 Val Acc: 98.65%\n",
      "[Epoch 5, Batch 300] Train Loss: 0.024 Train Acc: 99.16% Val Loss: 0.048 Val Acc: 98.62%\n",
      "[Epoch 5, Batch 400] Train Loss: 0.029 Train Acc: 99.09% Val Loss: 0.039 Val Acc: 98.88%\n",
      "[Epoch 5, Batch 500] Train Loss: 0.025 Train Acc: 99.28% Val Loss: 0.046 Val Acc: 98.53%\n",
      "[Epoch 5, Batch 600] Train Loss: 0.022 Train Acc: 99.19% Val Loss: 0.036 Val Acc: 98.87%\n",
      "[Epoch 5, Batch 700] Train Loss: 0.021 Train Acc: 99.30% Val Loss: 0.045 Val Acc: 98.72%\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(\n",
    "            device, dtype=torch.float32\n",
    "        )  # convert input tensor to float32\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy and loss\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print statistics\n",
    "        if i % 100 == 99:\n",
    "            # Compute validation accuracy and loss\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in valloader:\n",
    "                    images, labels = data\n",
    "                    images = images.to(\n",
    "                        device, dtype=torch.float32\n",
    "                    )  # convert input tensor to float32\n",
    "                    labels = labels.to(device)\n",
    "                    images, labels = data\n",
    "                    images = images.to(\n",
    "                        device, dtype=torch.float32\n",
    "                    )  # convert input tensor to float32\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch + 1}, Batch {i + 1}] \"\n",
    "                f\"Train Loss: {running_loss / 100:.3f} \"\n",
    "                f\"Train Acc: {100 * correct_train / total_train:.2f}% \"\n",
    "                f\"Val Loss: {val_loss / len(valloader):.3f} \"\n",
    "                f\"Val Acc: {100 * correct_val / total_val:.2f}%\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.038 Test Acc: 98.75%\n"
     ]
    }
   ],
   "source": [
    "# Compute test accuracy and loss\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {test_loss / len(testloader):.3f} \"\n",
    "    f\"Test Acc: {100 * correct_test / total_test:.2f}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
